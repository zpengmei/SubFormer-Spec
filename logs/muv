Random seed set as 42
/home/zpengmei/mambaforge/envs/pytorch/lib/python3.11/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
SubFormer(
  (local_mp): LocalMP(
    (atom_encoder): AtomEncoder(
      (embeddings): ModuleList(
        (0-8): 9 x Embedding(100, 256)
      )
    )
    (clique_encoder): Embedding(4, 256)
    (activation): ReLU()
    (back_activation): LeakyReLU(negative_slope=0.01)
    (bond_encoders): ModuleList(
      (0-2): 3 x BondEncoder(
        (embeddings): ModuleList(
          (0-2): 3 x Embedding(6, 256)
        )
      )
    )
    (graph_convs): ModuleList(
      (0-2): 3 x GINEConv(nn=Sequential(
        (0): Linear(in_features=256, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Linear(in_features=512, out_features=256, bias=True)
      ))
    )
    (graph_norms): ModuleList(
      (0-2): 3 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (sub_norms): ModuleList(
      (0-2): 3 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (atom2clique_lins): ModuleList(
      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
    )
    (clique2atom_lins): ModuleList(
      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
    )
    (clique): Linear(in_features=256, out_features=256, bias=True)
  )
  (pe): PositionalEncoding(
    (activation): ReLU()
    (deg_emb): Embedding(100, 256)
    (deg_lin): Linear(in_features=256, out_features=256, bias=True)
    (deg_merge): Linear(in_features=256, out_features=256, bias=True)
    (tree_lpe_lin): Linear(in_features=16, out_features=128, bias=True)
    (lpe_lin): Linear(in_features=16, out_features=128, bias=True)
  )
  (encoder): Encoder(
    (activation): ReLU()
    (encoder_layer): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (linear1): Linear(in_features=512, out_features=512, bias=True)
      (dropout): Dropout(p=0.2, inplace=False)
      (linear2): Linear(in_features=512, out_features=512, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.2, inplace=False)
      (dropout2): Dropout(p=0.2, inplace=False)
    )
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-3): 4 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=512, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (readout): Sequential(
    (0): Linear(in_features=768, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=17, bias=True)
  )
)

100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.74it/s]
Epoch: 001, Loss: 0.0213, Train: 0.0132, Val: 0.0107, Test: 0.1096
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.82it/s]
Epoch: 002, Loss: 0.0146, Train: 0.0598, Val: 0.0500, Test: 0.0703
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:23<00:00, 25.24it/s]
Epoch: 003, Loss: 0.0135, Train: 0.1211, Val: 0.0882, Test: 0.1866
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.13it/s]
Epoch: 004, Loss: 0.0126, Train: 0.1396, Val: 0.0729, Test: 0.1866
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.07it/s]
Epoch: 005, Loss: 0.0123, Train: 0.2136, Val: 0.0732, Test: 0.1866
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.09it/s]
Epoch: 006, Loss: 0.0118, Train: 0.2144, Val: 0.0600, Test: 0.1866
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:21<00:00, 26.54it/s]
Epoch: 007, Loss: 0.0111, Train: 0.2507, Val: 0.1102, Test: 0.1421
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.05it/s]
Epoch: 008, Loss: 0.0108, Train: 0.2874, Val: 0.0839, Test: 0.1421
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.95it/s]
Epoch: 009, Loss: 0.0100, Train: 0.3392, Val: 0.1192, Test: 0.1909
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.42it/s]
Epoch: 010, Loss: 0.0098, Train: 0.3547, Val: 0.1448, Test: 0.1548
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:21<00:00, 26.61it/s]
Epoch: 011, Loss: 0.0093, Train: 0.3945, Val: 0.1641, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.10it/s]
Epoch: 012, Loss: 0.0090, Train: 0.4095, Val: 0.1242, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.31it/s]
Epoch: 013, Loss: 0.0084, Train: 0.5155, Val: 0.1376, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.65it/s]
Epoch: 014, Loss: 0.0080, Train: 0.5407, Val: 0.0912, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.16it/s]
Epoch: 015, Loss: 0.0076, Train: 0.5764, Val: 0.1327, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.46it/s]
Epoch: 016, Loss: 0.0072, Train: 0.6044, Val: 0.1456, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:21<00:00, 26.83it/s]
Epoch: 017, Loss: 0.0068, Train: 0.6784, Val: 0.1019, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.37it/s]
Epoch: 018, Loss: 0.0064, Train: 0.6814, Val: 0.1165, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.28it/s]
Epoch: 019, Loss: 0.0059, Train: 0.7455, Val: 0.1376, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:23<00:00, 25.24it/s]
Epoch: 020, Loss: 0.0056, Train: 0.7471, Val: 0.1368, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.42it/s]
Epoch: 021, Loss: 0.0051, Train: 0.8332, Val: 0.1146, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:21<00:00, 26.51it/s]
Epoch: 022, Loss: 0.0046, Train: 0.8235, Val: 0.1363, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.58it/s]
Epoch: 023, Loss: 0.0043, Train: 0.8463, Val: 0.0914, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:20<00:00, 27.88it/s]
Epoch: 024, Loss: 0.0040, Train: 0.8647, Val: 0.1175, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.41it/s]
Epoch: 025, Loss: 0.0037, Train: 0.8850, Val: 0.1442, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:21<00:00, 27.01it/s]
Epoch: 026, Loss: 0.0033, Train: 0.9050, Val: 0.1418, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.64it/s]
Epoch: 027, Loss: 0.0028, Train: 0.9074, Val: 0.1034, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.99it/s]
Epoch: 028, Loss: 0.0030, Train: 0.9458, Val: 0.1502, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.23it/s]
Epoch: 029, Loss: 0.0026, Train: 0.9393, Val: 0.1237, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.87it/s]
Epoch: 030, Loss: 0.0027, Train: 0.9662, Val: 0.1249, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.18it/s]
Epoch: 031, Loss: 0.0022, Train: 0.9714, Val: 0.1475, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.57it/s]
Epoch: 032, Loss: 0.0021, Train: 0.9776, Val: 0.1324, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:24<00:00, 23.53it/s]
Epoch: 033, Loss: 0.0017, Train: 0.9717, Val: 0.1399, Test: 0.1740
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.31it/s]
Epoch: 034, Loss: 0.0016, Train: 0.9830, Val: 0.1684, Test: 0.2197
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.09it/s]
Epoch: 035, Loss: 0.0019, Train: 0.9673, Val: 0.1044, Test: 0.2197
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.64it/s]
Epoch: 036, Loss: 0.0021, Train: 0.9670, Val: 0.1449, Test: 0.2197
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:21<00:00, 26.88it/s]
Epoch: 037, Loss: 0.0014, Train: 0.9734, Val: 0.1267, Test: 0.2197
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.19it/s]
Epoch: 038, Loss: 0.0013, Train: 0.9737, Val: 0.1230, Test: 0.2197
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:21<00:00, 27.00it/s]
Epoch: 039, Loss: 0.0014, Train: 0.9569, Val: 0.1112, Test: 0.2197
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.89it/s]
Epoch: 040, Loss: 0.0012, Train: 0.9782, Val: 0.0971, Test: 0.2197
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.72it/s]
Epoch: 041, Loss: 0.0012, Train: 0.9889, Val: 0.1254, Test: 0.2197
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.87it/s]
Epoch: 042, Loss: 0.0008, Train: 0.9926, Val: 0.1451, Test: 0.2197
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.17it/s]
Epoch: 043, Loss: 0.0008, Train: 0.9955, Val: 0.2510, Test: 0.1826
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:21<00:00, 26.61it/s]
Epoch: 044, Loss: 0.0006, Train: 0.9941, Val: 0.2116, Test: 0.1826
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.35it/s]
Epoch: 045, Loss: 0.0006, Train: 0.9934, Val: 0.1544, Test: 0.1826
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 26.38it/s]
Epoch: 046, Loss: 0.0006, Train: 0.9981, Val: 0.1360, Test: 0.1826
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:22<00:00, 25.50it/s]
Epoch: 047, Loss: 0.0007, Train: 0.9879, Val: 0.1185, Test: 0.1826
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:21<00:00, 26.74it/s]
Epoch: 048, Loss: 0.0008, Train: 0.9956, Val: 0.1309, Test: 0.1826
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:21<00:00, 26.64it/s]
Epoch: 049, Loss: 0.0008, Train: 0.9919, Val: 0.1494, Test: 0.1826
100%|█████████████████████████████████████████████████████████████████████████████████| 582/582 [00:21<00:00, 26.96it/s]
Epoch: 050, Loss: 0.0007, Train: 0.9969, Val: 0.1196, Test: 0.1826